---
title: "NLP exam"
author: "Maris Sala"
date: "1/1/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

########### ----------------------------------------------------- #############
########### FIRST STEPS - READING IN WHOLE DATA, LIBRARIES, WKDIR #############
########### ----------------------------------------------------- #############

Read in the data, set the working directory, libraries
Initial data is the original Excel file where the data was collected to
```{r}
pacman::p_load(ggplot2,dplyr,gridExtra)

# Working directory
setwd("~/Aarhus University/NLP/NLP exam")

# The data
dat = read.csv("fb_data_R.csv", sep = "\t", encoding="UTF-8", stringsAsFactors=FALSE)

# Remove empty data
data = dat[ which( complete.cases(dat)) , ]
# Format the timestamp
data$Timestamp <- as.Date(data$Timestamp, format='%d/%m/%y')
# Add a column with Popularity
data$Popularity = data$Likes+data$Shares+data$Comments
data$Terrible = as.factor(data$Terrible)
```
Find some of the basic descriptors of the data for the popularity measures
```{r}
#Mean popularity
mean(data$Popularity)
# Median popularity
median(data$Popularity)
# Average likes
mean(data$Likes)
median(data$Likes)
# Average shares
mean(data$Shares)
median(data$Shares)
# Average comments
mean(data$Comments)
median(data$Comments)
```
########### ----------------------------------------------------- #############
###########             PART 1: POPULARITY ANALYSIS               #############
########### ----------------------------------------------------- #############

########### ----------------------------------------------------- #############
###########         PART 1A: TIMESERIES VISUALIZATION             #############
########### ----------------------------------------------------- #############

Plot posts with popularity. This is the initial investigation of whether post popularity is dependent on timestamp.
x-axis is time, y-axis is popularity

```{r echo = T}
# Timeseries plot for all of the data
g = ggplot(data, aes(x=Timestamp, y=Popularity)) +
  geom_point() +
  geom_smooth(method=lm, span = 0.3) +
  labs(title = "Popularity of post in time. Nov - Jan 2019") +
  theme_minimal()

# A hack to make the next plot - make a copy of the dataframe
data2 = data

# Make a popularity post based on radicality
b = ggplot() + 
  geom_point(subset(data2, Terrible == 1), mapping = aes(x = Timestamp, y = Popularity), color = "red") +
  geom_point(subset(data, Terrible == 0), mapping = aes(x = Timestamp, y = Popularity), color = "blue") +
  geom_smooth(subset(data2, Terrible == 1), mapping = aes(x = Timestamp, y = Popularity), color = "red", method=lm) +
  geom_smooth(subset(data, Terrible == 0), mapping = aes(x = Timestamp, y = Popularity), color = "blue", method=lm) +
  labs(title = "Popularity of radical (red) and non-radical (blue) posts over time. Nov 2019 - Jan 2020") +
  theme_minimal()

# Arrange the plots to compare the effect of time on all posts and posts based on radicality scores
grid.arrange(g,b)

```

########### ----------------------------------------------------- #############
########### PART 1B: PPOPULARITY DIFFERENCES BETWEEN RADICALITY   #############
########### ----------------------------------------------------- #############

Visualizing popularity differences of radical and non-radical posts with a violin plot. Getting numerical descriptors based on radicality of post.

```{r}
# Calculate the lengths of radical and non radical posts
n_t = length(which(data$Terrible == 0))
t = length(which(data$Terrible == 1))

# Violin plot of the popularity differences between radical and nonradical posts
ggplot(data, aes(y=Popularity, x=Terrible)) +
  geom_violin()  +
  labs(title = "Popularity differences between radical and non-radical content") +
  theme_minimal()

#adj_dat = data
#adj_dat$Adj_Popularity["Terrible" == 0] = 3
adj_dat$Adj = 0
adj_dat <- within(adj_dat, Adj[Terrible == 0] <- n_t)
adj_dat <- within(adj_dat, Adj[Terrible == 1] <- t)

#Adjusted violin plot
ggplot(adj_dat, aes(y=Popularity/Adj, x=Terrible)) +
  geom_violin()  +
  labs(title = "Popularity differences between radical and non-radical content") +
  theme_minimal()

## General numbers
# Find means of the 2 groups
data$Terrible = as.numeric(as.character(data$Terrible))
# Mean of terrible posts
mean(data[data$Terrible>0.1,"Popularity"])
# Mean of non-terrible posts
mean(data[data$Terrible<0.1,"Popularity"])

# Median of terrible posts
median(data[data$Terrible>0.1,"Popularity"])
# Median of non-terrible posts
median(data[data$Terrible<0.1,"Popularity"])

# Mean of terrible posts
median(data[data$Terrible>0.1,"Likes"])
# Mean of non-terrible posts
median(data[data$Terrible<0.1,"Likes"])

# Mean of terrible posts
median(data[data$Terrible>0.1,"Shares"])
# Mean of non-terrible posts
median(data[data$Terrible<0.1,"Shares"])

# Mean of terrible posts
median(data[data$Terrible>0.1,"Comments"])
# Mean of non-terrible posts
median(data[data$Terrible<0.1,"Comments"])
```

########### ----------------------------------------------------- #############
###########         PART 1C: TIME-RELATED CALCULATIONS            #############
########### ----------------------------------------------------- #############

PART 1C: Calculations: time between 2 radical posts and average popularity of posts
```{r}
# Make a subset first
radical = subset(data, select = c("Timestamp", "Terrible", "Popularity"))

# For the 1st calculations, I'll just take the Terrible == 1 to calculate time differences
rad_1 = subset(radical, Terrible==1)

# Calculated differences of days and popularity of posts
rad_2 = tail(rad_1, -1) - head(rad_1, -1)

# Average time difference
mean(rad_2$Timestamp)

# Average popularity change per post
mean(rad_2$Popularity)

# Average popularity per post
mean(rad_1$Popularity)

```
########### ----------------------------------------------------- #############
###########           PART 2: WORD CONTENT ANALYSIS               #############
########### ----------------------------------------------------- #############

PART 2: Post-by-post analysis
Here we want the text from the posts
1) General word frequency plot
2) Words and time plot
3) Relate words to popularity?
4) Or media/emoticons

########### ----------------------------------------------------- #############
###########       PART 2A: WORD FREQUENCY VISUALIZATIONS          #############
########### ----------------------------------------------------- #############
Get the frequent nounds from estnltk Jupyter notebook files. 

Noun frequency based on radicality plot
```{r}

rnouns = read.csv("estnltk/tutorials/nlp_pipeline/radical_nouns.txt", header = F, stringsAsFactors = F)

rn_freq = rnouns %>% group_by(V1) %>% count(V1)

# Create a new dataframe with the frequencies
#n_freq = count(nouns, 'V1')

# Choose frequencies
rnfreq2 = subset(rn_freq, rn_freq$n > 3)

rnfreq3 <- rnfreq2 %>% arrange(desc(n))

rnfreq3$n = as.factor(rnfreq3$n)

datax = as.data.frame(rnfreq3$V1)
names(datax)[1] <- "Word"
datax$Count = rnfreq3$n
datax$Count = as.numeric(as.character(datax$Count))
#datax$Count = as.character(datax$Count)
rdatax = datax

# Frequency plot
ggplot(datax, aes(x = Count, y = reorder(Word, Count))) +
  geom_point() +
  labs(title = "Frequency of nouns (35 most popular nouns)", y = "Words") +
  theme_minimal()

nrnouns = read.csv("estnltk/tutorials/nlp_pipeline/nonradical_nouns.txt", header = F, stringsAsFactors = F)

nrn_freq = nrnouns %>% group_by(V1) %>% count(V1)

# Create a new dataframe with the frequencies
#n_freq = count(nouns, 'V1')

# Choose frequencies
nrnfreq2 = subset(nrn_freq, nrn_freq$n > 10)

nrnfreq3 <- nrnfreq2 %>% arrange(desc(n))

nrnfreq3$n = as.factor(nrnfreq3$n)

datax = as.data.frame(nrnfreq3$V1)
names(datax)[1] <- "Word"
datax$Count = nrnfreq3$n
datax$Count = as.numeric(as.character(datax$Count))
#datax$Count = as.character(datax$Count)
ndatax = datax

# Frequency plot
ggplot(datax, aes(x = Count, y = reorder(Word, Count))) +
  geom_point() +
  labs(title = "Frequency of nouns (35 most popular nouns)", y = "Words") +
  theme_minimal()

ggplot() + 
  geom_point(data = rdatax, aes(x = Count/229, y = Word), color = "red") +
  geom_point(data = ndatax, aes(x = Count/633, y = Word), color = "blue") +
  labs(title = "Word usage in radical and non-radical posts", x = "Adjusted count") +
  theme_minimal()

length(nrn_freq$V1)

```


```{r}
nouns = read.csv("estnltk/tutorials/nlp_pipeline/all_nouns.txt", header = F, stringsAsFactors = F)

library(plyr)
pacman::p_load(tidyverse)

n_freq = nouns %>% group_by(V1) %>% count(V1)

# Create a new dataframe with the frequencies
#n_freq = count(nouns, 'V1')

# Choose frequencies
freq2 = subset(n_freq, n_freq$n > 10)

freq3 <- freq2 %>% arrange(desc(n))

freq3$n = as.factor(freq3$n)

datax = as.data.frame(freq3$V1)
names(datax)[1] <- "Word"
datax$Count = freq3$n
datax$Count = as.numeric(as.character(datax$Count))
#datax$Count = as.character(datax$Count)


# Frequency plot
ggplot(datax, aes(x = Count, y = reorder(Word, Count))) +
  geom_point() +
  labs(title = "Frequency of nouns (35 most popular nouns)", y = "Words") +
  theme_minimal()
```

PART 3: Radical 

```{r}
dat = read.csv("cumulative_sum_R.csv", sep = "\t", encoding="UTF-8", stringsAsFactors=FALSE)
# Remove empty data
dat2 = dat[ which( complete.cases(dat)) , ]
# Format the timestamp
dat2$Timestamp <- as.Date(dat2$Timestamp, format='%d/%m/%y')

sumdata = subset(dat2, Terrible == 1)
```


Fun plots
```{r}
pacman::p_load(caret,e1071)
data$Sad.content = as.factor(data$Sad.content)
data$Sad.reaction = as.factor(data$Sad.reaction)
ggplot(data, aes(Sad.content, Sad.reaction)) +
  geom_count()

confusionMatrix(data$Sad.reaction, reference = data$Sad.content)
```

# Make some models
```{r}
pacman::p_load(lmer,lme4)

OLSexamp <- lm(Popularity ~ Timestamp, data = data)
summary(OLSexamp)

OLSexamp <- lm(Popularity ~ Timestamp + Terrible, data = data)
summary(OLSexamp)

OLSexamp <- lm(Shares ~ Terrible + Sad.reaction, data = data)
summary(OLSexamp)

data$Sad.content = as.numeric(as.character(data$Sad.content))
data$Sad.reaction = as.numeric(as.character(data$Sad.reaction))

OLSexamp <- lm(Sad.reaction ~ Sad.content, data = data)
summary(OLSexamp)

OLSexamp <- lm(Popularity ~ Cumulative.sum, data = sumdata)
summary(OLSexamp)

OLSexamp <- lm(Shares ~ Cumulative.sum, data = sumdata)
summary(OLSexamp)

```

Popularity of post and popular words
```{r}
popular = subset(data, Popularity > mean(data$Popularity))
unpopular = subset(data, Popularity < (mean(data$Popularity)-70))

write.csv(popular, file = "popular_posts.csv", fileEncoding = "UTF-8")
write.csv(unpopular, file = "unpopular_posts.csv", fileEncoding = "UTF-8")
```

Read the POS files back in and find what types we have
```{r}
dat = read.csv("estnltk/tutorials/nlp_pipeline/morph_pos_popular.txt", header = F, stringsAsFactors = F)

# Remove punctuation and spaces
dat$V1 = gsub('[[:punct:]]',"",dat$V1)
dat$V1 = gsub('\\s', "", dat$V1)

# Summarise the data
pos_data = dat %>%
  group_by(V1) %>%
  summarize(n())

## Same for unpopular
dat2 = read.csv("estnltk/tutorials/nlp_pipeline/morph_pos_unpopular.txt", header = F, stringsAsFactors = F)

# Remove punctuation and spaces
dat2$V1 = gsub('[[:punct:]]',"",dat2$V1)
dat2$V1 = gsub('\\s', "", dat2$V1)

# Summarise the data
un_pos_data = dat2 %>%
  group_by(V1) %>%
  summarize(n())

colnames(pos_data) <- c("POS_tag", "Count")
colnames(un_pos_data) <- c("POS_tag", "Count")

length(dat$V1)
length(dat2$V1)

# Plot these on the same plot
ggplot() +
  geom_point(data = pos_data, aes(x = POS_tag, y = Count/2791), color = "deeppink1", size = 5, shape = 18) +
  geom_line(data = pos_data, aes(x = POS_tag, y = Count/2791), color = "deeppink1", group = 1) +
  geom_point(data = un_pos_data, aes(x = POS_tag, y = Count/2040), color = "darkblue", size = 5, shape = 17) +
  geom_line(data = un_pos_data, aes(x = POS_tag, y = Count/2040), color = "darkblue", group = 1) +
  labs(title = "Word type counts between popular (diamond) and unpopular (triangle) posts", x = "POS tag", y = "Adjusted count") +
  theme_minimal()
```

Analyse unique words
```{r}
pop_unique = read.csv("estnltk/tutorials/nlp_pipeline/unique_popular.txt", header = F, stringsAsFactors = F)
### SAME FOR UNPOPULAR NOUNS
unpop_unique = read.csv("estnltk/tutorials/nlp_pipeline/unique_unpopular.txt", header = F, stringsAsFactors = F)

# Sum them up
un_un = unpop_unique %>%
  group_by(V1) %>%
  summarize(n())

pop_un = pop_unique %>%
  group_by(V1) %>%
  summarize(n())

colnames(un_un) <- c("Word", "Count")
colnames(pop_un) <- c("Word", "Count")
# Frequency plot
ggplot(un_un, aes(x = Count, y = reorder(Word, Count))) +
  geom_point() #+
  labs(title = "Frequency of nouns (35 most popular nouns)", y = "Words") +
  theme_minimal()
  
ggplot(pop_un, aes(x = Count, y = reorder(Word, Count))) +
  geom_point() #+
  labs(title = "Frequency of nouns (35 most popular nouns)", y = "Words") +
  theme_minimal()
```

Attempt co-occurrence analysis :))
```{r}
options(stringsAsFactors = FALSE)
pacman::p_load(quanteda)
library(quanteda)

textdata <- subset(data, select = c(Timestamp, Content))
colnames(textdata) = c("date", "text")
textdata <- tibble::rowid_to_column(textdata, "doc_id")

# Make a subset of the 2 posts?
#textdata = subset(textdata[1:2,])
sotu_corpus <- corpus(textdata$text, docnames = textdata$doc_id, docvars = data.frame(year = substr(textdata$date, 0, 4)))

# original corpus length and its first document
ndoc(sotu_corpus)

substr(texts(sotu_corpus)[1], 0, 200)

corpus_sentences <- corpus_reshape(sotu_corpus, to = "sentences")

ndoc(corpus_sentences)

texts(corpus_sentences)[1]

texts(corpus_sentences)[2]

############################################################################################################

# Read in the lemmatized posts?
pbp = read.csv("post_by_post_lemmas.csv", encoding="UTF-8", stringsAsFactors=FALSE)

# Remove punctuation and spaces
pbp$inflicted_form = gsub('[[:punct:]]',"", pbp$inflicted_form)
pbp$lemma = gsub('[[:punct:]]', "", pbp$lemma)
pbp$inflicted_form = gsub('\\s', "", pbp$inflicted_form)
pbp$lemma = gsub('\\s', "", pbp$lemma)
# Make inflicted form lowercase
pbp$inflicted_form = tolower(pbp$inflicted_form)

# Making a test dafarame just in case
test = pbp

# Remove empty strings
test = test[which(test[,2]>1),]

# Now I have the pure lemmas, could I do some analysis?
############################################################################################################

# Build a dictionary of lemmas
lemma_data <- test#read.csv("resources/baseform_en.tsv", encoding = "UTF-8")

# read an extended stop word list
#stopwords_extended <- readLines("resources/stopwords_en.txt", encoding = "UTF-8")
#No stopwords

# Preprocessing of the corpus of sentences
corpus_tokens <- corpus_sentences %>% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>% 
  tokens_tolower() %>% 
  tokens_replace(lemma_data$inflected_form, replacement = NULL, valuetype = "fixed") #%>% 
  #tokens_remove(pattern = stopwords_extended, padding = T)

# calculate multi-word unit candidates
sotu_collocations <- textstat_collocations(corpus_tokens, min_count = 1)
sotu_collocations <- sotu_collocations[1:250, ]

corpus_tokens <- tokens_compound(corpus_tokens, sotu_collocations)

minimumFrequency <- 10

# Create DTM, prune vocabulary and set binary values for presence/absence of types
binDTM <- corpus_tokens %>% 
  tokens_remove("") %>%
  dfm() %>% 
  dfm_trim(min_docfreq = minimumFrequency, max_docfreq = Inf) %>% 
  dfm_weight("boolean")

# Matrix multiplication for cooccurrence counts
coocCounts <- t(binDTM) %*% binDTM

as.matrix(coocCounts[202:205, 202:205])


coocTerm <- "uus"
k <- nrow(binDTM)
ki <- sum(binDTM[, coocTerm])
kj <- colSums(binDTM)
names(kj) <- colnames(binDTM)
kij <- coocCounts[coocTerm, ]


# Read in the source code for the co-occurrence calculation
source("calculateCoocStatistics.R")
# Definition of a parameter for the representation of the co-occurrences of a concept
numberOfCoocs <- 15
# Determination of the term of which co-competitors are to be measured.
coocTerm <- "california"

```

Trying things out on my own now
```{r}
# Read in the lemmatized posts?
pbp = read.csv("post_by_post_lemmas.csv", encoding="UTF-8", stringsAsFactors=FALSE)

# Remove punctuation and spaces
pbp$text = gsub('[[:punct:]]',"", pbp$text)
pbp$text = gsub('\\s', "", pbp$text)

# Making a test dafarame just in case
test = pbp

# Remove empty strings
test = test[which(test[,2]>1),]

#####################################
# Extract the ID's of popular and unpopular posts
data = tibble::rowid_to_column(data, "doc_id")
# 30 popular and 30 unpopular posts
popular <- data[with(data,order(-Popularity)),][1:30,]
unpopular <- data[with(data,order(Popularity)),][1:30,]

# Extract the document ID's
popular_id = popular$doc_id
unpopular_id = unpopular$doc_id

#####################################
# Make a list of lists of popular posts
popular_id

subset(test, test$doc_id == popular_id)

test[test$doc_id == popular_id,]

test$doc_id = as.factor(test$doc_id)
split(test, doc_id)

# A list of lists (more or less) where 
thesplits = split(test, with(test, interaction(doc_id)), drop = TRUE)

thesplits$'56'["text"] # this gets me the words in post 56
foo = thesplits[56]

popular_posts = 0
for (i in popular_id){
  print(i)
  popular_posts[i] = subset(test, test$doc_id==i)
}


#####################################

# Make the posts into lists
list1 = subset(test, test$doc_id==0)
list2 = subset(test, test$doc_id==1)

list1 = list1$text
list2 = list2$text

# Remove duplicates
list1 = list1[!duplicated(list1)]
list2 = list2[!duplicated(list2)]

####################################


####################################
#Another solution: create a dataframe where you list all the words in one column, and the other columns show whether this word is present in the posts at hand
df = data.frame("words" = lemma_data$lemma)
# Remove duplicates
library(tidyverse)
df = df %>% distinct()
df$Post1 = 0
df$Post2 = 0


# Now I need to compare the 1st post word list to the words in column 1 of df


# Match list to words to find the positions that need to be updated to 1 from 0
wholematch = match(list2, df$words)

for (i in wholematch){
    df$Post2[i] = 1
}

df$Freq = df$Post1 + df$Post2

# Looping this as
for (x in list_of_lists){
  wholematch = match(x, df$words)
  
  for (i in wholematch){
    df$Post[]
  }
}

df$Freq = df$Post1 + df$Post2
```

Popular radical vs unpopular radical
```{r}
datx = subset(data, Terrible == 1)
datx$Terrible = as.numeric(as.character(datx$Terrible))

sum(datx$Terrible)

Rad_popular <- datx[with(datx,order(-Popularity)),][1:7,]
Rad_unpopular <- datx[with(datx,order(Popularity)),][1:7,]

## Attention: bad programming ahead :/// (I don't have a good solution for this right now but there should be some kind of loop instead of the following mess)
Rad_popular$doc_id
list1 = subset(test, test$doc_id==101)$text
list2 = subset(test, test$doc_id==55)$text
list3 = subset(test, test$doc_id==62)$text
list4 = subset(test, test$doc_id==56)$text
list5 = subset(test, test$doc_id==21)$text
list6 = subset(test, test$doc_id==90)$text
list7 = subset(test, test$doc_id==86)$text

Rad_unpopular$doc_id
list8 = subset(test, test$doc_id==32)$text
list9 = subset(test, test$doc_id==70)$text
list10 = subset(test, test$doc_id==40)$text
list11 = subset(test, test$doc_id==77)$text
list12 = subset(test, test$doc_id==23)$text
list13 = subset(test, test$doc_id==89)$text
list14 = subset(test, test$doc_id==111)$text

# Now that the ugly yet oddly aesthetic part is over, let's see the word frequency plot
# Almost
Rad_df <- data.frame("word" = matrix(unlist(list1), byrow=T),stringsAsFactors=FALSE)
Rad_df <- rbind(Rad_df, data.frame("word" = matrix(unlist(list2), byrow=T),stringsAsFactors=FALSE))
Rad_df <- rbind(Rad_df, data.frame("word" = matrix(unlist(list3), byrow=T),stringsAsFactors=FALSE))
Rad_df <- rbind(Rad_df, data.frame("word" = matrix(unlist(list4), byrow=T),stringsAsFactors=FALSE))
Rad_df <- rbind(Rad_df, data.frame("word" = matrix(unlist(list5), byrow=T),stringsAsFactors=FALSE))
Rad_df <- rbind(Rad_df, data.frame("word" = matrix(unlist(list6), byrow=T),stringsAsFactors=FALSE))
Rad_df <- rbind(Rad_df, data.frame("word" = matrix(unlist(list7), byrow=T),stringsAsFactors=FALSE))
Rad_pop = Rad_df

Rad_df <- data.frame("word" = matrix(unlist(list8), byrow=T),stringsAsFactors=FALSE)
Rad_df <- rbind(Rad_df, data.frame("word" = matrix(unlist(list9), byrow=T),stringsAsFactors=FALSE))
Rad_df <- rbind(Rad_df, data.frame("word" = matrix(unlist(list10), byrow=T),stringsAsFactors=FALSE))
Rad_df <- rbind(Rad_df, data.frame("word" = matrix(unlist(list11), byrow=T),stringsAsFactors=FALSE))
Rad_df <- rbind(Rad_df, data.frame("word" = matrix(unlist(list12), byrow=T),stringsAsFactors=FALSE))
Rad_df <- rbind(Rad_df, data.frame("word" = matrix(unlist(list13), byrow=T),stringsAsFactors=FALSE))
Rad_df <- rbind(Rad_df, data.frame("word" = matrix(unlist(list14), byrow=T),stringsAsFactors=FALSE))
Rad_un = Rad_df

# Now frequencies
Rad_pop_freq = Rad_pop %>% group_by(word) %>% count(word)

# Create a new dataframe with the frequencies
#n_freq = count(nouns, 'V1')

# Choose frequencies
freq2 = subset(Rad_pop_freq, Rad_pop_freq$n > 3)

freq3 <- freq2 %>% arrange(desc(n))

freq3$n = as.factor(freq3$n)

datax = as.data.frame(freq3$word)
datax$Count = freq3$n
datax$Count = as.numeric(as.character(datax$Count))
#datax$Count = as.character(datax$Count)
rdatax = datax

Rad_un_freq = Rad_un %>% group_by(word) %>% count(word)

# Create a new dataframe with the frequencies
#n_freq = count(nouns, 'V1')

# Choose frequencies
freq2 = subset(Rad_un_freq, Rad_un_freq$n > 3)

freq3 <- freq2 %>% arrange(desc(n))

freq3$n = as.factor(freq3$n)

datax = as.data.frame(freq3$word)
datax$Count = freq3$n
datax$Count = as.numeric(as.character(datax$Count))
#datax$Count = as.character(datax$Count)
ddatax = datax


colnames(rdatax) = c("Word", "Count")
colnames(ddatax) = c("Word", "Count")

length(rdatax$Word)
length(ddatax$Word)
# Word frequency plot
ggplot() + 
  geom_point(data = rdatax, aes(x = Count/19, y = Word), color = "red") +
  geom_point(data = ddatax, aes(x = Count/11, y = Word), color = "blue") +
  labs(title = "Word usage in radical and non-radical posts", x = "Adjusted count") +
  theme_minimal()

```

